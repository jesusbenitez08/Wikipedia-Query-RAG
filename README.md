Reflection Report

For this project I picked the Wikipedia article on Willem Dafoe as the main source. Since I think the article covers his life, acting career, major roles, awards, and personal background. Since it has a mix of career highlights and detailed background, it believe it had a lot of material to test and it actually worked well for testing in general.

5 Deep Dive Questions with the answers the AI gave me:
What are some of Willem Dafoe’s popular roles?
He’s widely known for Platoon, The Last Temptation of Christ, Shadow of the Vampire, and The Florida Project. All of these got him major praise, and some led to Oscar nominations.

How did he get into acting?
He started in experimental theater—especially The Wooster Group in NYC—before transitioning into film.

Who did he play in Spiderman, and why is that important?
He played Norman Osborn (aka Green Goblin). It became one of his most iconic roles and made him a household name in pop culture.

How many Oscar nominations does he has and for what films?
He’s been nominated four times—for Platoon, Shadow of the Vampire, The Florida Project, and At Eternity’s Gate.

What is his take on acting?
He’s all about intensity and depth. He often picks unconventional roles and fully commits to them, working closely with directors to build unique characters.

What I Noticed While Testing
Retrieval Quality Based on Chunk Size & Overlap
Small chunks (like 200 words) were fast but not always helpful for full answers. Sometimes the info was split in between chunks.

Larger chunks (600–800 words) with some overlap (50–100 words) gave much better results, a lot more context and less missed details.

No overlap at all, there were no miss connections between ideas that were just a few sentences apart in the source.

How Good Were the Answers?
Pretty accurate overall, especially when the chunk size and overlap were tuned right.

I think they were pretty well written, the AI gave fluent, clear answers that were easy to understand.

Some little mistakes, like occasionally, it filled in gaps with assumptions when it couldn’t find a perfect or exact match in the text.

Ideas to Make It Better:
Add more sources, like film interviews or IMDb summaries to give the AI a better pool of info.

Rank the results better. Some of the top answers weren’t actually the most relevant. A smarter updated or reranking step could help.

Handle pronouns in a smarter way. Especially with long chunks, the AI sometimes lost track of who “he” or “the actor” was.

Get real feedback. Human feedback or a better structured evaluation could give better insight on quality.

Make it user more user friendly. A small web app where users type a question and see answers with the sources would be a nice extension.
